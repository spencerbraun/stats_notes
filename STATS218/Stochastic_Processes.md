---
title: Stochastic Processes II
date: 20200407
author: Spencer Braun
---

[TOC]

# Stochastic Processes II

## Introduction

* We have an experiment -> set of possible outcomes called the sample space $\Omega$
* An event is a subset of outcomes
* Axioms of probability
	* $0 \leq P(E) \leq 1$ for all events E (P(E) = probability of E)
	* $P(\Omega) = 1$
	* If $E_1,E_2,...$ disjoint events, then $P(\cup_{i=1}^\infty E_i) = \sum_{i=1}^\infty P(E_i)$. Disjoint means $E_i \cap E_j = \empty$ whenever $i\neq j$. Probability of the union is the sum of probabilities for disjoint events.
* RV is a function from $\Omega$ into $\R$. 
* A stochastic process is a collection of RVs $(X_t)_{t \in T} =X$. T can be $\{1,2,3,...\}$ or $[0,\infty)$; a sequence or continuum of RVs.

## Chapter 3: Renewal Theory
* Let $X_1,X_2,...$ be iid nonnegative RVs. Assume $P(X_n = 0 ) < 1$, there is some chance that our RV is nonzero. Let $\mu = E(X_n)$ where we allow $\mu$ to be $\infty$, but we have a well-defined expected value.
	* For example, if X is integer-valued and $P(X = k) = \frac{c}{k^2}$ where $c \sum \frac{1}{k^2} = 1$ then $E(X) = \infty$
* These variables are called **interarrival times**. Imagine clients arriving at a server, $X_i =$ time between $(i-1)th$ and ith arrival. Clients are immediately served (no queueing involved). Let $S_0= 0, \; S_n = \sum_{i=1}^n X_i$. Then $S_n=$ arrival time of the nth client.
* Let $N(t) = sup\{n: S_n \leq t\}$ (sup since theoretically there could be infinitely many) = number of clients arriving by time t. Maximum n such that the nth client has arrived by time t.
* The stochastic process $(N(t))_{t \geq 0}$ is called a **renewal process**. Since the interarrival times are iid, it follows that at each rewnwal the process probabilistically starts over. Think of replacing lightbulbs - it stays on for a certain time before being replaced. N(t) is the number of bulbs you have to replace by time t. Plotted, we see piecewise constant function, jumping to the next integer i at each $S_i$. The times at which the jumps happen are random. A Poisson process is an example of a renewal process, limited to interarrival times with exponential random variables.
* Let $F_n(t) = P(S_n \leq t)$. This function is a CDF of a sum of RVs $S_n$, may not be easy to evaluate but can still be quite useful. Observe that $N(t) \geq n \iff S_n \leq t$; the number of variables up to the time t iff the nth customer has arrived by time t. Therefore $P(N(t) =n) = P(N(t) \geq n) - P(N(t) \geq n + 1) = P(S_n \leq t ) - P(S_{n+1} \leq t) = F_n(t) - F_{n+1}(t)$. If we know the CDF then we can calculate the probabilities for N(t).
* **Renewal function**: expected number of variables up to time t. The renewal function $m: [o, \infty)\rightarrow \R$ is defined as $m(t) = E(N(t) )=$ expected number of arrivals by time t.
* Generally we observe the renewal process for a while, observe the distribution of interarrival times then try to say something about what we can expect in the future.
* <u>**Theorem**</u> (Proposition 3.2.1, 3.2.2): $m(t) = \sum_{n=1}^\infty F_n(t)$ and $m(t) < \infty$ for all t.
  * We can get F from the interarrival times, then calculate m and get the expected number of arrivals in some interval i in the future. m(t) is finite no matter what distribution we use. 
  * **Lemma 1**: Let X be a non-negative integer valued RV with $E(X) < \infty$ then $E(X) = \sum_{n=1}^\infty P(X \geq n)$
    * Proof: $E(X) = P(X = 1) + 2P(X=2) + 3P(X=3)$. Regrouping $=[P(X=1) + P(X=2) + ...] + [P(X=2) + P(X=3) + ...]+[P(X=3) + P(X=4) + ...] + ...$ This sum is equal to $\sum_{n=1}^\infty P(X \geq n)$. When summing non-negative quantities, we can rearrange the sum in any way and still get the same answer.
  * **Lemma 2** (Markov's Inequality): Let X be a non negative RV with $E(X) < \infty$ then for any $t > 0,\; P(X\geq t) \leq \frac{E(X)}{t}$
    * The LHS of the inequality is often much more complicated that the RHS, so this comes in handy
    * Proof: Let y be the following RV: $y = \begin{cases} 1 & if \;x \geq t \\ 0 & else \end{cases}$. Let Z = X/t. Then if X < t we have $y = 0 \leq Z$ since Z is a nonnegative RV. If $X \geq t$ then $y = 1 \leq x / t = Z$. So y is always $\leq Z \implies E(y) \leq E(Z)$. Finally, $E(y) = 0P(y = 0) + 1P(y=1) = P(X \geq t)$ and $E(Z) = \frac{E(X)}{t}$
  * **Lemma 3**: Let X be a non-negative RV st $P(X = 0) < 1$, then $E(e^{-X}) < 1$.
  	*  Note if X were always 0 then $e^{-X}$ would always be 1. 
    * Proof: Suppose $E(e^{-X}) = 1$ then $E(1-e^{-X}) = 0$. Since X is nonnegative then $1 - e^{-X} \geq 0$. So  $1 - e^{-X}$ is a nonnegative RV whose expectation is 0. So by Markov's Inequality, $P(1 - e^{-X} \geq t) \leq 0$ for any $t > 0 \implies P(1 - e^{-X} =0) = 1\implies P(X = 0)=1$. So we get a contradiction. Thus $E(e^{-X}) \neq 1$. But $e^{-X} < 1$ always so $E(e^{-X})$ cannot be > 1 thus  $E(e^{-X}) < 1$
  * **Lemma 4**: $m(t) < \infty$ for any t
    * Proof: $P(N(t) \geq n) = P(S_n \leq t)$ by above. $= P(e^{-S_n} \geq e^{-t}) \leq \frac{E(e^{-S_n})}{e^{-t}}$. Using the fact that X's are iid, then $ = e^t (E(e^{-X_1})^N) = e^tp^n$. By Lemma 1 $p = E(e^{X_1})$. Then $m(t) = E(N(t)) = \sum_{n=1}^\infty P(N(t) \geq n) = \sum_{n=1}^\infty P(S_n \leq t = \sum_{n=1}^\infty) F_n(t)$. By the above $\sum_{n=1}^\infty P(N(t) \geq n) \leq e^t \sum_{n=1}^\infty p^n < \infty$ since $0 \leq p < 1$.
* Note the above also shows $E\left[N^{r}(t)\right]<\infty$ for all $t ,r \geq 0$
* Recall: $\mu = E(X_1) = E(X_2)=...$ which could be infinite.

### Limit Theorems

* **<u>Theorem</u>** (Proposition 3.3.1): With probability 1, $\underset{t\rightarrow \infty}{lim} \frac{N(t)}{t} = \frac{1}{\mu}$
  * We will use the Strong Law of Large Numbers (SLLN) which says that $\frac{S_n}{n} \rightarrow \mu$ with probability 1. Means that the $P(\underset{t\rightarrow \infty}{lim} \frac{S_n}{n} = \mu) = 1$. This holds for mu finite or infinite.
  * **Lemma**: $\underset{t\rightarrow \infty}{lim} N(t) = \infty$ with probability 1.
  	* As we take larger times, the number of arrivals goes to infinity. The only way in which $N(\infty)$, the total number of renewals that occurs, can be finite is for one of the interarrival times to be infinite (the next arrival never occurs).
  	* Proof: The limit always exists because N is an increasing process. $\underset{t\rightarrow \infty}{lim} N(t) < \infty \iff X_n = \infty$  for some n. This is an integer valued process, so finite if it stops at some point. $P(X_n = \infty) = 0$ so $P(\underset{t\rightarrow \infty}{lim} N(t) < \infty) = p(\cup_{n=1}^\infty \{X_n = \infty\}) \leq \sum_{n=1}^\infty P(X_n = \infty) = 0$ (Probability of union is bounded by sum of probabilities). Fact: for any events $A_1,A_2,...\; P(\cup_{n=1}^\infty A_i )\leq \sum_{n=1}^\infty P(A_i)$
  * Proof: Note $S_{N(t)} \leq t < S_{N(t) + 1} $. The customer we count up to for N(t) arrives by t but the next customer must be after t by the definition of N(t). Recall $S_n$ is the arrival time of the nth client, N(t) = # of arrivals by time t. This implies $\frac{S_{N(t)}}{N(t) } \leq \frac{t}{N(t)} < \frac{S_{N(t) + 1}}{N(t)}$. $\frac{S_n}{N(t)}$ is the average of the first N(t) interarrival times, and by using our SLLN fact that $N(t) \rightarrow \infty,\; \frac{S_n}{n} \rightarrow \mu$ as $n \rightarrow \infty$ we get  $\frac{S_{N(t)}}{N(t)} \rightarrow \mu$ as $t \rightarrow \infty$. This all happens with probability 1.
  * Then $\frac{S_{N(t)}}{N(t)} = \frac{N(t) + 1}{N(t)}\frac{S_{N(t)}}{N(t) + 1} \rightarrow 1\times \mu $ as $t \rightarrow \infty$. Therefore $\frac{t}{N(t)} \rightarrow \mu$ as $t \rightarrow \infty$
* Note $S_{N(t)}$ is the time of the last renewal prior to or at time t. Therefore $S_{N(t) + 1}$ is the time of the first renewal after time t.
* $\frac{1}{\mu}$ is the rate of the renewal process. By Prop 3.3.1, with probability 1 the long-run rate at which renewals occur is equal to $\frac{1}{\mu}$. 
* **Stopping Time**: Let $X_1,X_2,...$ denote a sequence of RVs. An integer-valued RV N is said to be a stopping time for the sequence if the event $\{N=n\}$ is independent of $X_{n+1},X_{n+2},...$ for all n and N is positive integer valued.  Said another way, the occurrence or non-occurrence of event $\{N=n\}$ is completely determined by the values of $X_1,X_2,...$ for every n. 
  * Note we allow $N = \infty$ also. Whether you stop or not, our evaluation of whether to stop is completely deterministic at time n.
  * Example: Let N = $min\{n: X_1+...+X_n \geq 5\}$. First n so the sum is at least 5. Then the event $\{N = n\}$ can be rewritten as the event $\{X_1 < 5, X_1+X_2 < 5,..., X_1+...+X_{n-1} < 5, X_1+...+X_{n} \geq 5\}$ . Up to n-1, our sum is always less than 5, then at n, the sum is greater or equal to 5. Rewritten this way, it is clear that the values of $X_1,...,X_n$ completely determine whether $\{N=n\}$ has happened or not, making N a stopping time for this sequence of X's.
* **<u>Theorem 3.3.2: Wald's Equation</u>**: If $X_1,X_2,...$ are iid RVs having finite expecations ($E|X_i|<\infty$), and if N is a stopping time for $X_1,X_2,...$ st $E(N) < \infty$ then $E\left[\sum_{i}^{N} X_{n}\right]=E[N ] E[X_1]$
  * Normally with a finite sum, the expectation would just move inside. Here we have a finite sum, but the range of the sum in random (see 217 for theorems on random sums). 
  * Fact: If $Y_1,Y_2,...$ are RVs st $\sum_{i=1}^\infty E|Y_i|  < \infty$ then $E(\sum_{i=1}^\infty Y_i) = \sum_{i=1}^\infty E(Y_i)$ (Consequence of dominated convergence theorem)
  * Proof of Wald: Let $I_n = \begin{cases} 1 & if \; N \geq n \\ 0 & else \end{cases}$ . Let $Y_n = X_n I_n = \begin{cases} X_n& if \; N \geq n \\ 0 & else \end{cases}$. Then $\sum_{n=1}^N X_n = \sum_{n=1}^\infty Y_n$ since $Y_n =0 $ for $ N \geq n$. We will show that $\sum_{n=1}^\infty E|Y_n| < \infty$ then we will conclude that $E(\sum_{n=1}^N X_n) = E(\sum_{n=1}^\infty Y_n) = \sum_{n=1}^\infty E(Y_n)$. Finally we will show that $\sum_{n=1}^\infty E(Y_n) = E(N)E(X_1)$
    * First note that the value of $I_n $ is determined by the occurrence or non-occurrence of the event $\{N < n\}$. This event $\{N < n\} = \{N =1\} \cup \{N =2\} \cup ...\cup \{N =n-1\}$. The occurrence or non-occurrence of each of the events on the right can be determined by the values of $X_1,...,X_{n-1}$. Therefore $I_n$ itself is a function of $X_1,..,X_{n-1}$.  Since these variables are independent, we conclude that $X_n,I_n$ are independent. 
    * So $E|Y_n| = E|X_nI_n| = E(|X_n||I_n|) = E|X_n| E(I_n) \underset{iid}{=} E|X_1| E(I_n)$. Then $=E|X_1|P(N \geq n)$. So in summation, $\sum_{n=1}^\infty E|Y_n| = E|X_1|\sum_{n=1}^\infty P(N \geq n) = E|X_1|E(N) < \infty$. 
    * Thus $E(\sum_{n=1}^N X_n) = E(\sum_{n=1}^\infty Y_n) = \sum_{n=1}^\infty E(Y_n)$
    * But again, $E(Y_n) = E(X_n I_n) \overset{\perp}{=} E(X_n)(I_n) = E(X_1) P(N \geq n) $ So $\sum_{n=1}^\infty E(Y_n) = E(X_1) \sum_{n=1}^\infty P(N\geq n) = E(X_1) E(N)$
  * Example: Stopping time $E(N) = \infty$ and Wald's equation fails. Let $X_1,...$ iid with $P(X_1=1) = P(X_1 = -1) = 1/2$. Let $S_n = \sum_{i=1}^n X_i$, $S_0 = 0$. let $N = min\{n: S_n = 1\}$. This is a random walk, looking for first time sum equal to 1. The event $\{N=n\} = \{S_1 \neq 1,....,S_{n-1} \neq 1, S_n = 1\}$, so N is a stopping time but Wald's equation does not hold because $E(N)E(X_1) = 0$ since $E(X_1) = 0$ but $E(\sum_{i=1}^N X_i) = E(S_N) = 1$ since $S_N = 1$. We can prove that the sum will definitely hit 1 at some point. 
  * Example: Let $X_1,X_2,...$ iid $P(X_i = 1) = P(X_i = -1) = 1/2$. Let $T = \begin{cases} 1 & if \; X_1 > X_1 + X_2 \\ 2 &  if \; X_1 \leq X_1 + X_2 \end{cases}$ and $S_n = \sum_{i=1}^n X_i$. Stop at 1 if somehow you know in the future this will yield you more money but continue to step 2 otherwise. T is not a stopping time since requires future knowledge. For all possible combinations of $X_1, X_2$, $E(S_T) = 1/2 \neq E(T)E(X_1) = 0$. Wald's lemma fails because T is not a stopping time.
* **<u>Theorem (Elementary Renewal Theorem)</u>**: $\underset{t \rightarrow \infty}{lim} \frac{m(t)}{t} = \frac{1}{\mu}$
  * Note that $ \frac{m(t)}{t} = E( \frac{N(t)}{t})$, so it looks like we have this result from above. However, $X_n \rightarrow a $ does not imply $E(X_n)\rightarrow a$ so we must prove this result.
  * This tells us about the behavior of the expected number of variables. Blackwell's renewal theorem is a more useful version of this. 
  * **Lemma**: $X_1,X_2,...$ iid interarrival times. Take any $ t \geq 0$ then $N(t) + 1$ is a stopping time with respect to the sequence of X's.
    * Proof: $\{N(t) + 1 = n\} = \{N(t) = n-1\} = \{X_1+...X_{n-1} \leq t, X_1+...X_{n} > t\}$. The nth variable happens after time t, but the n-1st variable happens before or at t.
  * **Corrolary** 3.3.3: $E(S_{N(t) + 1}) = \mu(m(t) + 1)$ if $\mu < \infty$
    * Proof: We have shown that $E(N(t) + 1) = E(N(t) + 1) = m(t) + 1 < \infty$. So by Wald's equation, $E(S_{N(t) + 1}) = E(\sum_{i=1}^{N(t) + 1} X_i) = E(N(t) + 1)E(X_1) = \mu(m(t) + 1)$
  * Proof: First we will show that $\underset{t \rightarrow \infty}{liminf} \frac{m(t)}{t} \geq \frac{1}{\mu}$. If $\mu < \infty$ then by the corr, $\frac{\mu(m(t) + 1)}{t} = \frac{E(S_{N(t) + 1 })}{t}$, the arrival time of the N(t) + 1 customer over t, which we know have to be bigger than 1 since the arrival always happens after t. But $S_{N(t) + 1} > t \implies E(S_{N(t) + 1} ) > t$ always. This implies $\frac{\mu(m(t) + 1)}{t} > 1 \implies \underset{t \rightarrow \infty}{liminf} \frac{m(t)}{t} \geq \frac{1}{\mu}$. If $\mu = \infty$, then this is trivially true.
  	* Next step: show $\underset{t \rightarrow \infty}{limsup}   \frac{m(t)}{t} \leq \frac{1}{\mu}$. Take some constant $M > 0$, then define $\bar{X}_n = \begin{cases} X_n & if \; X_n \leq M \\ M & if X_n > M\end{cases}$. $\bar{S_n} = \sum_{i=1}^n \bar{X_i},\; \mu_M = E(\bar{X}_1),\; \bar{N}(t) = sup \{n: \bar{S_n} \leq t \},\; \bar{m}(t) = E(\bar{N}(t))$. We can note that $\bar{S}_{\bar{N}(t) + 1} \leq t + M$- if the interarrival times are bounded by M, the arrival time of the customer that arrives right after t cannot exceed t by more than M. This is a nice result of truncation - we are supplied with an upper bound. Recall $\bar{N}(t) + 1$ is a stopping time and $E(\bar{N}(t) + 1 ) = \bar{m}(t) + 1 < \infty$ so by Wald, $(\bar{m}(t)+1) \mu_{M} \leq t+M \implies \frac{\bar{m}(t)}{t}+\frac{1}{t} \leq \frac{t+M}{t \mu_M}$. Take limsup of both sides $\implies \limsup _{t \rightarrow \infty} \frac{\bar{m}(t)}{t} \leqslant \frac{1}{\mu_{M}}$. 
  	* Note that the nth customer in the new process always arrives before the nth customer in the old process since the interarrival times are equal or smaller. The new process has every interrival time equal or less than the old process for all n. Thus $\bar{S}_n \leq S_n,\; \forall n$ and so $\bar{N}(t) \geq N(t)$ for all t. This implies that $\bar{m}(t) \leq m(t) \implies \underset{t \rightarrow \infty}{limsup} \frac{m(t)}{t} \leq  \underset{t \rightarrow \infty}{limsup} \frac{\bar{m}(t)}{t} \leq \frac{1}{\mu_M} $. The LHS does not depend on M, so the proof will be complete if we can show that $\mu_M \rightarrow \mu$ as $M \rightarrow \infty$. This follows by a measure theoretic results known as the monotone convergence theorem. 

### Key Renewal Theorem and Applications

* A nonnegative RV X is said to be **lattice** if there exists $d \geq 0$ st $\Sigma_{n=0}^{\infty} P\{X=n d\}=1$ - it only takes on integral multiples of some nonnegative number d. In other words, $X \in \{0, d, 2d, ...\}$ with probability 1. The largest d is said to be the **period** of X. If X is lattice and F is the distribution function of X, then we say F is lattice. 
* **<u>Blackwell's Theorem</u>**: (i) If F is not lattice, then $m(t+a)-m(t) \rightarrow a / \mu$ as $t \rightarrow \infty$ for all $a \geq 0 $.  (ii) If F is lattice with period d, then E(number of arrivals at nd) $\rightarrow d/\mu$ as $n \rightarrow \infty$
  * F is the CDF of the interarrival times.
  * If F is not lattice (say X continuous), the expected number of renewals in an interval of length a far from the origin is approx $a / \mu$ - the further away from the origin we are, the less influence initial effects will have. 
  * When F is lattice with period d, then the limit $g(a) \equiv \lim _{t \rightarrow \infty}[m(t+a)-m(t)]$ cannot exist. Renewals can only occur at integral multiples of d, so the expected number of renewals in an interval far from the origin depends on how many points of the for $nd$ it contains. If interarrivals are always positive, then part (ii) says $\lim _{n \rightarrow \infty} P\{\text { renewal at } n d\}=\frac{d}{\mu}$ 
  * For a PP, m(t) is exactly linear equal to $\lambda t$ , so its derivative wrt t is always $\lambda$. In general, m(t) may start out wiggly, but for very large t, m(t) will look approximately linear.
* Let h be a function defined on $[0, \infty) \rightarrow \R$. For any a > 0, le $\underline{h}_n(a)$ be the supremum and $\overline{h}_n(a)$ the infinum of h(t) over the interval $(n-1)a \leq t \leq na$. We say that h is directly Riemann integrable if $\sum_{n=1}^\infty \overline{h}_n(a)$ and $\sum_{n=1}^\infty \underline{h}_n(a)$ are finite for all a > 0. Basically these sums quickly go to zero for these sums to be finite. Also $\lim _{a \rightarrow 0} a \sum_{n=1}^{\infty} \bar{h}_{n}(a)=\lim _{a \rightarrow 0} a \sum_{n=1}^{\infty} h_{n}(a)$. A sufficient condition for h to be directly Riemann integrable is that 
  * $h(t) \geq 0,\; \forall t \geq 0$  - non negative
  * h(t) is nonincreasing
  * $\int_{0}^{\infty} h(t) d t<\infty$
* **<u>Key Renewal Theorem</u>**: If F is not lattice and if h(t) is directly Riemann integrable then $\lim _{t \rightarrow \infty} \int_{0}^{t} h(t-x) d m(x)=\frac{1}{\mu} \int_{0}^{t} h(t) d t$ where $m(x)=\sum_{n_{1}}^{\infty} F_{n}(x)$ and $\mu=\int_{0}^{\infty} \overline{F}(t) d t$. 
	* Aside: $\int ... dm(x)$ is the Riemann-Stieltjes integral wrt m. Usual Riemann integral $\int_a^b f(x) dx = lim \sum_{i=1}^n f(x_i)(x_{i+1} - x_i)$ as you take finer and finer partitions $x_0,...,x_n$ of [a,b]. The RS integral $\int_a^b f(x) d g(x)$ is the limit of $\sum_{i=1}^n f(x_I)(g(x_{i+1} - g(x_i)))$. If g is differentiable, then since $g(X_{i+1} - g(x_i)) \approx g^\prime(x_i)(x_{i+1} - x_I)$ we get $\int_a^b f(x) d g(x) = \int_a^b f(x) g^\prime(x) dx$
  * Blackwell and KRT can be shown to be equivalent. 
  * Intuition: If m is differentiable, $ \int_{0}^{t} h(t-x) d m(x) =  \int_{0}^{t} h(t-x) m^\prime(x) dx$. Now h(t-x) is very small unless x is close to t since h decays rapidly (see conditions on h). So we can ignore the part of the integral where x is far away from t. OTOH, if t is large and x is close to t, Blackwell's thm implies that $m^\prime(x) \approx \frac{1}{\mu}$. So if x close to t and large, $\int_{0}^{t} h(t-x) m^\prime(x) dx \approx \int_0^t h(t-x) \frac{1}{\mu}dx = \frac{1}{\mu}\int_0^t h(y) dy \approx \frac{1}{\mu}\int_0^\infty h(y) dy $ if t large.
  * KRT is used when one wants to compute the limiting value of g(t), so probability or expectation at time t. Generally derive an equation for g(t) by conditioning on the time of the last renewal prior to t, which yields equations of the form $g(t)=h(t)+\int_{0}^{t} h(t-x) d m(x)$. 
  * **Lemma 3.4.3**: $P\left\{S_{N(t)} \leq s\right\}=\overline{F}(t)+\int_{0}^{t} \overline{F}(t-y) d m(y)$ for $t \geq s \geq 0$

### Alternating Renewal Processes

* Example: Lightbulb replacements. Want to know the distribution of the amount of time that a lightbulb remains lit. Say you have an inspector that comes in over long periods of time and records how long the bulb has been on. If most are very short lived but a few will live for a while, the inspector will not see the short lived bulbs typically - he will see the longer lasting bulbs since they are installed the majority of the time (the others are replaced quickly, despite their majority). KRT can make this distribution precise.
* Basic formula: Let X be nonneg RV with cdf F and let A be an event. Then $P(A \cap \{X \leq s\}) = \int_0^s P(A|X=t)dF(t)$. 
	* Proof sketch: $P(A \cap \{X \leq s\}) = \sum_{i=1}^n P(A  \cap \{s_i < X \leq s_{i+1}\})$ where $0 = s_0 \leq s_1 \leq ...\leq s_n =s$. This can be written as $  \sum_{i=1}^n P(A  | s_i < X \leq s_{i+1})P(s_i < X \leq s_{i+1})$ and $P(s_i < X \leq s_{i+1}) = F(s_{i+1} - F(s_i))$. The $s_i$ are finer and finer partitions - x lies in a small interval so $P(A  | s_i < X \leq s_{i+1}) \approx P(A  |X= s_i )$ so the sum approaches this integral. When $F^\prime = f$, then $P(A \cap  \{X \leq s\}) = \int_0^s P(A|X= t) f(t) dt$ generally.
* Using this idea, let's define some distributions. Consider process of replacing lightbulbs, let $A(t) = t - S_{N(t)} = $ age of the bulb that is on at time t, since $S_{N(t)}$ is the time of the last replacement. Then $Y(t) = S_{N(t) + 1} - t = $ residual life of the bulb that is on at time t. Then $X_{N(t) + 1} = $ total lifetime of the bulb that is on at time t. Therefore $X_{N(t) + 1} = A(t) + Y(t) $. On a numberline, have a time t with $S_{N(t)}$ to the left and the space between them is A(t), while the space from t to $S_{N(t) + 1}$ to the right is Y(t). The gap from $S_{N(t)}$  to $S_{N(t) + 1}$ is  $X_{N(t) + 1}$. 
	* We will compute the limiting distribution of A(t), Y(t), and $X_{N(t) + 1}$ as $t \rightarrow \infty$. A(t) we can calculate from data and use renewal theory for the other distributions.
	* The expected number of arrivals in a short period of time far in the future is $\frac{1}{\mu}$ times the length of the interval
* **Distribution of A(t)**: $P(A(t) \leq x) = ?$ for $0 \leq x < t$. This event $\{A(t) \leq x\}$ is the disjoint union of the events $\{t - x \leq S_n \leq t, S_{n+1} > t\}, n = 1,2,...$. If this event happens then these other events must happen; and if one of these events happens then $A(t) \leq x$. The nth event is that the lightbulb burns out between $t-x,\; t $ and the n+1st lightbulb burns out after t. 
	* $A(t) \leq x$ means that exactly one of the following must have happened: either the first lightbulb goes out between time t-x and t $S_1 \in [t-x, t], \; S_2 > t$ and the second lightbulb goes out after t OR the second lightbulb out between t - x and t and third goes out after t  $S_2 \in [t-x, t], \; S_3 > t$...etc. Disjoint events
  * So $P(A(t) \leq x) = \sum_{n=1}^\infty P(t - x \leq S_n \leq t, S_{n+1} > t)$ - the probability of A(t) is the same as exactly one of these events has happened. 
  * Then $P(t - x \leq S_n \leq t, S_{n+1} > t) = \int_{t-x}^t P(S_{n+1} > t | S_n = y) d F_n(y)$, recalling $F_n$ is CDF of $S_n$. (Eg. if $F_n$ differentiable and derivative is the pdf of $S_n$ then $P(t-x \leq S_n \leq t, S_{n+1} > t) = \int_{t-x}^t P(S_{n+1} > t | S_N = y) f_n(y) dy$. This is a generalization of that.) 
  * So we can write $P(A(t) \leq x ) = \sum_{n=1}^\infty \int_{t-x}^t P(S_{n+1} > t | S_n = y) dF_n(y)$.
  * General measure-theoretic results imply that the infinite sum can be taken inside the integral: $P(A(t) \leq x ) = \int_{t-x}^t  \sum_{n=1}^\infty P(S_{n+1} > t | S_n = y) dF_n(y)$.
  * Then (for $y \leq t$) $ P(S_{n+1} > t | S_n = y)  = P(x_{n+1} > t - y | S_N = y) =  P(x_{n+1} > t-y )$ since $X_{n+1} and S_n$ are independent, so conditioning goes away. $P(x_{n+1} > t-y )= \bar{F}(t-y)$ where $\bar{F}(n) = 1 - F(n)$ and F is the CDF of the interarrival times.
  * Therefore $P(A(t) \leq x = \int_{t-x}^t\bar{F}(t-y) \sum_{n=1]^\infty dF_n(y)}))$. In a R-S integral $\int f(x) d g(x)+\int f(x) d h(x) = \int f(x) d(g+h)(x) = \int f(x) dw(x)$ for $w = g+h$. Extends to infinite sums under conditions. We know that $m(t) = \sum_{n=1}^\infty F_n(t)$. Thus $\sum_{n=1}^{\infty} d F_{n}(y)=\operatorname{dm}(y)$.
	* Then $P(A(t) \leqslant x)=\int_{t-x}^{t} \bar{F}(t-y) d m(y) = \int_0^t h(t-y)dm(y)$. Define $h(u) = \begin{cases} \bar{F}(u) & if \; u \leq x \\ 0 & else \end{cases}$. Since y < t - x implies t - y > x implies h(t-y) = 0.
	* Returning to the Key Renewal Thm, the h that we defined satisfies the sufficient conditions for KRT, so we can invoke the KRT to get the limit of our probability. h is nonnegative, non increasing, and integral is finite. Assume F is non lattice. 
	* From KRT we get $\lim _{t \rightarrow \infty} P(A(t) \leqslant x) = \frac{1}{\mu} \int_{0}^{\infty} h(u) d u = \frac{1}{\mu} \int_{0}^{x} \bar{F}(u) d u$. Therefore the limiting CDF of A(t) as $t \rightarrow \infty$ is given by $G(u) = \frac{1}{\mu} \int_{0}^{x} \bar{F}(u) d u$. By FTC, this is a differentiable function: $G^\prime(x) = \frac{1}{\mu}\bar{F}(x) =  \frac{1}{\mu}(1 - F(x))$. Why is this a valid PDF? Because $\mu = E(X_1) = \int(_0^\infty) P(X_1 > t ) dt = \int_0^\infty \bar{F}(t) dt$ (we did not prove but is a fact: we can write expectations as infinite integrals). 
* Example: $X_i \sim exp(\lambda$ (we have a PP). Then $F(x) = 1 - e^{-\lambda x}$ and $\mu =lambda$. Then $G(x) = \lambda e^{-\lambda x}$, as $t \rightarrow \infty$  then $A(t) \sim Exp(\lambda)$.
	* If instead $X_i \sim Unif(0,1)$ then $F(x) = x ,\; x \in [0,1],\; \mu = 1/2$. Then $G(x) = 2(1-x)$. Our pdf is then more likely to be close to 0 then towards 1 - not uniformly distributed.
* **Limiting Distribution of Y(t)**, the residual lifetime of the bulb that is on at time t.
	* Want to understand $P(Y(t) \leq x) = ?$. Events $\{Y(t) \leq x\}$ is the union of disjoint events $\left\{S_{n} \leq t, \quad t<S_{n+1} \leqslant t+x\right\} \quad n=0,1,2,...$. Either the first lightbulb goes off between t and t + x OR second lightbulb goes off between t and t + x and first goes off up to t...etc. This means $P(Y(t) \leq x) = \sum_{n=0}^\infty P\left(S_{n} \leq t, t<S_{n+1} \leqslant  t+x\right) = P(t < S_1 \leq t + x) + \sum_{n=1}^\infty \int_0^t P(t < S_{n+1} \leq t + x | S_n=y)d F_n(y)$. 
	* So the first term $ = \bar{F}(t) - \bar{F}(t + x)$ for $\bar{F}(y) = P(X_1 > y)$. Then in total have $\bar{F}(t) - \bar{F}(t + x) + \sum_{n=1}^\infty \int_0^t P(t-y < x_{n+1} \leq t + x - y | S_n=y)dF_n(y)$. Note $S_1 = X_1$ and $S_{n+1} = S_n + X_n$. So $= \bar{F}(t) - \bar{F}(t + x) + \sum_{n=1}^\infty \int_0^t P(t-y < x_{n+1} \leq t + x - y )dF_n(y)$ and by iid X we have $P(t-y < x_{n+1} \leq t + x - y ) = \bar{F}(t-y)-\bar{F}(t+x-y)$.
	* Then moving the sum inside $= \bar{F}(t) - \bar{F}(t + x) + \int_0^t  \bar{F}(t-y)-\bar{F}(t+x-y) \sum_{n=1}^\infty  dF_n(y)$ and noting $\sum_{n=1}^\infty  dF_n(y) = dm(y)$: $P(Y(t) \leq x)=  \bar{F}(t) - \bar{F}(t + x)  + \int_0^t h(t-y)dm(y)$ where $h(u) =  \bar{F}(u) - \bar{F}(u + x)$.  Notice that $\bar{F}(t) \rightarrow 0, \; t \rightarrow \infty$
	* So by KRT, $\underset{t \rightarrow \infty}{lim} P(Y(t) \leq x) = \frac{1}{\mu}\int_0^\infty h(u) du =  \frac{1}{\mu}\int_0^\infty ( \bar{F}(u) - \bar{F}(u + x)) du$. The splitting into two: $=   \frac{1}{\mu}[\int_0^\infty\bar{F}(u) du - \int_0^\infty \bar{F}(u + x) du] = \frac{1}{\mu}[\int_0^\infty\bar{F}(u) du - \int_x^\infty \bar{F}(y) dy]$. So in total get in the limit $=  \frac{1}{\mu} \int_0^x \bar{F}(u)du$. This is the same limit as the one we got for A(t)!
	* The age and the residual time have the same distribution. For our uniform example, both the age and the residual time are closer to 0 than to 1!
* **Limiting distribution of $X_{N(t) + 1}$ ** - the total lifetime of the bulb that is on at time t.
	* Look at event $\left\{x_{N(t)+1}>x\right\}$ is the disjoint union of the events $\{\left\{s_{n} \leq t, \quad s_{n+1}>t, x_{n+1}>x\right\}\}$ for n = 0, 1, 2.... 
	* Taking $t > x \geq 0$,  $P(x_{N(t)+1}>x) = \bar{F}(t) + \sum_{n=1}^\infty P(S_{n} \leqslant t, S_{n+1}>t, X_{n+1} > x)$ Then $ = \bar{F}(t) + \sum_{n=1}^\infty \int_0^t P(S_{n+1}>t, X_{n+1} > x|S_n =y) dF_n(y)$.
	* Suppose $S_n = y$ - there are two cases to consider. (1) $t-x \leq y \leq t$ and (2) $y < t-x$. Case 1: in this case, $x_{n+1} > x \implies S_{n+1} > t$ and then $P(S_{n+1}>t, X_{n+1} > x|S_n =y)  = P(X_{n+1} > x|S_n =y) \overset{\perp}{=} P(X_{n+1} > x) = \bar{F}(x)$.  In Case 2: in this case $S_{n+1} > t \implies x_{n+1} > x$. So $P(S_{n+1}>t, X_{n+1} > x|S_n =y) = P(S_{n+1}>t |S_n =y)  = P(X_{n+1} > t-y|S_n =y)  \overset{\perp}{=}P(X_{n+1} > t-y) = \bar{F}_n(t-y)$.
	* Combining these cases, you get $P(x_{N(t)+1}>x)  =   \bar{F}(t) +  \int_0^t \sum_{n=1}^\infty P(S_{n+1}>t, X_{n+1} > x|S_n =y) dF_n(y) =  \bar{F}(t)  +  \int_0^t h(t-y) \sum dF_n(y) $ where $\sum dF_n(y)  = dm(y)$. Here $h(u) = \begin{cases} \bar{F}(x) & if \; u \leq x \\ \bar{F}(u) & if \; u > x  \end{cases}$. 
	* By KRT, $\underset{t \rightarrow \infty}{lim} P\left(X_{N(t)+1}>n\right)=\frac{1}{N} \int_{0}^{\infty} h(u) d u = \frac{x \bar{f}(u)}{\mu} + \frac{1}{\mu}\int_x^\infty \bar{F}(u) du$. 

